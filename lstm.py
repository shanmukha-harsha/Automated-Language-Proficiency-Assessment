# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qOrsloQ3yunuaXsBTn4QU4cAmpWZmPjF
"""

!pip install -q transformers

# Commented out IPython magic to ensure Python compatibility.
def initialize():
  import re
  import warnings
  import numpy as np
  import pandas as pd
  import seaborn as sns
  import tensorflow as tf
  from wordcloud import WordCloud
  from matplotlib import pyplot as plt
  from transformers import TFAutoModel, AutoTokenizer
  from sklearn.feature_extraction.text import CountVectorizer
  from tensorflow.keras.preprocessing.text import Tokenizer
  from tensorflow.keras.preprocessing.sequence import pad_sequences
  train_df = pd.read_csv('train.csv')
  test_df = pd.read_csv('test.csv')

  input_essay = "It is very important for every child to get an education, as education can change their life. It will prepare the child for the harmonious transformation from a student into a responsible citizen. Thus, in order to make a better life for the children, education is mandatory for everyone. Therefore, the government should make education free for everyone. There are also some negative results if the education in colleges and universities is made free. There are some students, after completing their Higher Secondary, who opt for a professional course or vocational course. These students are aware of their aptitudes and have grown up. Some of them are interested in business, others opt for general courses. If education is made free in colleges and universities, then more students will be interested in taking up general courses. This can further result in unemployment, as there will be fewer job opportunities for the huge number of qualified students. The government can tackle this strong competition by adopting different methods of admitting the student. One of the methods is Entrance- Exams, where the students will be allowed to enter on the basis of their merit. The shortage in the job opportunity will lead the educated and highly qualified to settle down in low-level jobs, where they do not get their deserved honour for their capability and hard work. Thus the people will get demoralised and would get diverted from the idea of getting higher education. This would further put the image of free university education down in the eyes of new aspirants."
  i = 0
  test_df.loc[len(test_df.index)] = ["000BAD50D026"+str(i), input_essay ]
  i+=1
  train_df['full_text'] = train_df["full_text"].replace(re.compile(r'[\n\r\t]'), '', regex=True)
  test_df['full_text'] = test_df["full_text"].replace(re.compile(r'[\n\r\t]'), '', regex=True)

  train_df['num_words'] = train_df['full_text'].apply(lambda x: len(x.split()))
  avg_words = round(train_df['num_words'].mean())
  max_words = round(train_df['num_words'].max())
  print('Average length: {}'.format(avg_words))
  print('Max length: {}'.format(max_words))

  warnings.filterwarnings("ignore", category=FutureWarning)
  sns.set_theme()
#   %matplotlib inline

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

test_data = train_df[3850:3911]

test_data

train_df = train_df[0:2000]

train_df.shape

train_df.describe().style.background_gradient(cmap='Blues').set_properties(**{'font-family':'Segoe UI'})
train_df['full_text'][5]

train_df['full_text'] = train_df["full_text"].replace(re.compile(r'[\n\r\t]'), '', regex=True)
test_df['full_text'] = test_df["full_text"].replace(re.compile(r'[\n\r\t]'), '', regex=True)

train_df['num_words'] = train_df['full_text'].apply(lambda x: len(x.split()))
avg_words = round(train_df['num_words'].mean())
max_words = round(train_df['num_words'].max())
print('Average length: {}'.format(avg_words))
print('Max length: {}'.format(max_words))

cv = CountVectorizer(stop_words = 'english')
words = cv.fit_transform(train_df['full_text'])
sum_words = words.sum(axis=0)

words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]
words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)

wordcloud = WordCloud(width = 2000, height = 2000).generate_from_frequencies(dict(words_freq))

def tokenize():
  tokenizer = Tokenizer(oov_token="<OOV>")
  tokenizer.fit_on_texts(train_df['full_text'])
  word_index = tokenizer.word_index

  train_seq = tokenizer.texts_to_sequences(train_df['full_text'])
  pad_train = pad_sequences(train_seq, maxlen=max_words, truncating='post')

  test_seq = tokenizer.texts_to_sequences(test_df['full_text'])
  pad_test = pad_sequences(test_seq, maxlen=max_words, truncating='post')

tokenizew

word_idx_count = len(word_index)
print(word_idx_count)

X = pad_train
y = [train_df['cohesion'].values, train_df['syntax'].values, train_df['vocabulary'].values,
      train_df['phraseology'].values, train_df['grammar'].values, train_df['conventions'].values]

def get_lstm_model():

    inputs = tf.keras.layers.Input(shape=(max_words))
    embeddings = tf.keras.layers.Embedding(word_idx_count + 1, 64, input_length=max_words)(inputs)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embeddings)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))(x)
    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(x)

    output1 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(x)
    output1 = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(output1)
    output1 = tf.keras.layers.Dense(1, activation='relu', name='cohesion')(output1)


    output2 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(x)
    output2 = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(output2)
    output2 = tf.keras.layers.Dense(1, activation='relu', name='syntax')(output2)

    output3 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(x)
    output3 = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(output3)
    output3 = tf.keras.layers.Dense(1, activation='relu', name='vocabulary')(output3)

    output4 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(x)
    output4 = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(output4)
    output4 = tf.keras.layers.Dense(1, activation='relu', name='phraseology')(output4)

    output5 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(x)
    output5 = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(output5)
    output5 = tf.keras.layers.Dense(1, activation='relu', name='grammar')(output5)

    output6 = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(x)
    output6 = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                                    bias_initializer='zeros',
                                    kernel_regularizer=tf.keras.regularizers.L1L2(l1=1e-5, l2=1e-4),
                                    bias_regularizer=tf.keras.regularizers.L2(1e-4),
                                    activity_regularizer=tf.keras.regularizers.L2(1e-5))(output6)
    output6 = tf.keras.layers.Dense(1, activation='relu', name='conventions')(output6)

    model = tf.keras.models.Model(inputs=inputs, outputs=[output1, output2, output3, output4, output5, output6])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(),
        loss = tf.keras.losses.mean_squared_error
    )

    return model

model = get_lstm_model()

history = model.fit(X, y, epochs=30, validation_split=0.2, shuffle=True, verbose=1)

!mkdir -p saved_model2
model.save('saved_model2/my_model')

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
files.download('/content/saved_model2/my_model/variables/variables.data-00000-of-00001')

!zip -r /content/saved_model2.zip /content/saved_model2/

files.download('/content/saved_model2.zip')

initialize()
tokenize()

def load_model():
  new_model = tf.keras.models.load_model('drive/MyDrive/LSTM_Model/my_model')
  return new_model
#Check its architecture
#new_model.summary()

saved_model = load_model()

saved_model.summary()

test_df

input_essay = "It is very important for every child to get an education, as education can change their life. It will prepare the child for the harmonious transformation from a student into a responsible citizen. Thus, in order to make a better life for the children, education is mandatory for everyone. Therefore, the government should make education free for everyone. There are also some negative results if the education in colleges and universities is made free. There are some students, after completing their Higher Secondary, who opt for a professional course or vocational course. These students are aware of their aptitudes and have grown up. Some of them are interested in business, others opt for general courses. If education is made free in colleges and universities, then more students will be interested in taking up general courses. This can further result in unemployment, as there will be fewer job opportunities for the huge number of qualified students. The government can tackle this strong competition by adopting different methods of admitting the student. One of the methods is Entrance- Exams, where the students will be allowed to enter on the basis of their merit. The shortage in the job opportunity will lead the educated and highly qualified to settle down in low-level jobs, where they do not get their deserved honour for their capability and hard work. Thus the people will get demoralised and would get diverted from the idea of getting higher education. This would further put the image of free university education down in the eyes of new aspirants."

i = 0
test_df.loc[len(test_df.index)] = ["000BAD50D026"+str(i), input_essay ]
i+=1

test_df

print(pad_test.shape)

maxlen = 1250
sequences = tokenizer.texts_to_sequences([""])
data = pad_sequences(sequences, maxlen=maxlen)

data

X_test = data
results = new_model.predict(X_test)

results

abc = pd.DataFrame()
abc['text_id'] = test_df['text_id']
base = 0.5
for i in range(6):
    abc[i] = pd.Series([base * round(float(x)/base) for x in results[i]])
abc.columns = train_df.drop(columns=['full_text', 'num_words']).columns
abc = abc.clip(upper=pd.Series({'cohesion':4.0, 'syntax':4.0, 'vocabulary':4.0, 'phraseology':4.0, 'grammar':4.0, 'conventions':4.0}), axis=1)
abc['text_id'] = test_df['text_id']

abc.head()

test_seq = tokenizer.texts_to_sequences(test_df.iloc[-1]['full_text'])
pad_test = pad_sequences(test_seq, maxlen=max_words, truncating='post')

test_df['full_text']

print(X_test)

import math

X_test = pad_test
results = saved_model.predict(X_test)
lstm_prediction = pd.DataFrame()
lstm_prediction['text_id'] = test_df['text_id']
base = 0.5
for i in range(6):
    lstm_prediction[i] = pd.Series([base * round(float(x)/base) for x in results[i]])
lstm_prediction.columns = train_df.drop(columns=['full_text', 'num_words']).columns
lstm_prediction = lstm_prediction.clip(upper=pd.Series({'cohesion':4.0, 'syntax':4.0, 'vocabulary':4.0, 'phraseology':4.0, 'grammar':4.0, 'conventions':4.0}), axis=1)
lstm_prediction['text_id'] = test_df['text_id']

lstm_prediction.head()

average = (lstm_prediction.iloc[-1]['cohesion'] + lstm_prediction.iloc[-1]['syntax'] + lstm_prediction.iloc[-1]['vocabulary'] + lstm_prediction.iloc[-1]['phraseology'] + lstm_prediction.iloc[-1]['grammar'] + lstm_prediction.iloc[-1]['conventions'])/6
print(average)

lstm_submission.to_csv('lstmsubmission.csv', index=False)